Options in Toil
===============

A quick way to see all of Toil's CLI options is by executing::

    $ toil --help

Commandline Options
-------------------

**Core Toil Options**

  jobStore              The location of the job store for the workflow. A job
                        store holds persistent information about the jobs and
                        files in a workflow. If the workflow is run with a
                        distributed batch system, the job store must be
                        accessible by all worker nodes. Depending on the
                        desired job store implementation, the location should
                        be formatted according to one of the following
                        schemes: file:<path> where <path> points to a
                        directory on the file systen aws:<region>:<prefix>
                        where <region> is the name of an AWS region like us-
                        west-2 and <prefix> will be prepended to the names of
                        any top-level AWS resources in use by job store, e.g.
                        S3 buckets. azure:<account>:<prefix>
                        google:<project_id>:<prefix> TODO: explain For
                        backwards compatibility, you may also specify ./foo
                        (equivalent to file:./foo or just file:foo) or /bar
                        (equivalent to file:/bar).
  --workDir WORKDIR     Absolute path to directory where temporary files
                        generated during the Toil run should be placed. Temp
                        files and folders will be placed in a directory
                        toil-<workflowID> within workDir (The workflowID is
                        generated by Toil and will be reported in the workflow
                        logs. Default is determined by the variables (TMPDIR,
                        TEMP, TMP) via mkdtemp. This directory needs to exist
                        on all machines running jobs.
  --stats               Records statistics about the toil workflow to be used
                        by 'toil stats'.
  --clean {always,onError,never,onSuccess}
                        Determines the deletion of the jobStore upon
                        completion of the program. Choices: 'always',
                        'onError','never', 'onSuccess'. The --stats option
                        requires information from the jobStore upon completion
                        so the jobStore will never be deleted withthat flag.
                        If you wish to be able to restart the run, choose
                        'never' or 'onSuccess'. Default is 'never' if stats is
                        enabled, and 'onSuccess' otherwise
  --cleanWorkDir {always,never,onSuccess,onError}
                        Determines deletion of temporary worker directory upon
                        completion of a job. Choices: 'always', 'never',
                        'onSuccess'. Default = always. WARNING: This option
                        should be changed for debugging only. Running a full
                        pipeline with this option could fill your disk with
                        intermediate data.
  --clusterStats FILEPATH
                        If enabled, writes out JSON resource usage statistics
                        to a file. The default location for this file is the
                        current working directory, but an absolute path can
                        also be passed to specify where this file should be
                        written. This options only applies when using scalable
                        batch systems.
  --restart             If --restart is specified then will attempt to restart
                        existing workflow at the location pointed to by the
                        --jobStore option. Will raise an exception if the
                        workflow does not exist

**Logging Options**

Toil hides stdout and stderr by default except in case of job failure.  Log levels in toil are based on priority from
the logging module, so:

**DEBUG**: All log statements are shown.
**INFO**: All log statements are shown, except DEBUG.
**WARN**: Only WARN, ERROR, and CRITICAL log levels are shown.
**ERROR**: Only ERROR, and CRITICAL log levels are shown.
**CRITICAL**: Only CRITICAL log levels are shown.

    ``--logOff`` - Only CRITICAL log levels are shown.  Equivalent to ``--logLevel=OFF`` or ``--logLevel=CRITICAL``.

    ``--logCritical`` - Only CRITICAL log levels are shown.  Equivalent to ``--logLevel=OFF`` or ``--logLevel=CRITICAL``.

    ``--logError`` - Only ERROR, and CRITICAL log levels are shown.  Equivalent to ``--logLevel=ERROR``.

    ``--logWarning`` - Only WARN, ERROR, and CRITICAL log levels are shown.  Equivalent to ``--logLevel=WARNING``.

    ``--logInfo`` - All log statements are shown, except DEBUG.  Equivalent to ``--logLevel=INFO``.

    ``--logDebug`` - All log statements are shown.  Equivalent to ``--logLevel=DEBUG``.

    ``--logLevel=LOGLEVEL`` - ``LOGLEVEL`` may be set to: ``OFF`` (or ``CRITICAL``), ``ERROR``, ``WARN`` (or ``WARNING``), ``INFO``, or ``DEBUG``.

    ``--logFile FILEPATH`` - Specifies a file path to write the logging output to.

    ``--rotatingLogging`` - Turn on rotating logging, which prevents log files from getting too big (set using ``--maxLogFileSize BYTESIZE``).

    ``--maxLogFileSize BYTESIZE`` - Sets the maximum log file size in bytes (``--rotatingLogging`` must be active).

**Batch System Options**

  --batchSystem BATCHSYSTEM
                        The type of batch system to run the job(s) with,
                        currently can be one of LSF, Mesos, Slurm, Torque,
                        HTCondor, singleMachine, parasol, gridEngine'.
                        default=singleMachine
  --disableHotDeployment
                        Should hot-deployment of the user script be
                        deactivated? If True, the user script/package should
                        be present at the same location on all workers.
                        default=false
  --parasolCommand PARASOLCOMMAND
                        The name or path of the parasol program. Will be
                        looked up on PATH unless it starts with a
                        slashdefault=parasol
  --parasolMaxBatches PARASOLMAXBATCHES
                        Maximum number of job batches the Parasol batch is
                        allowed to create. One batch is created for jobs with
                        a a unique set of resource requirements. default=1000
  --scale SCALE         A scaling factor to change the value of all submitted
                        tasks's submitted cores. Used in singleMachine batch
                        system. default=1
  --linkImports         When using Toil's importFile function for staging,
                        input files are copied to the job store. Specifying
                        this option saves space by sym-linking imported files.
                        As long as caching is enabled Toil will protect the
                        file automatically by changing the permissions to
                        read-only.
  --mesosMaster MESOSMASTERADDRESS
                        The host and port of the Mesos master separated by
                        colon. (default: 169.233.147.202:5050)

**Autoscaling Options**

  --provisioner CLOUDPROVIDER
                        The provisioner for cluster auto-scaling. The
                        currently supported choices are'aws' or 'gce'. The
                        default is None.
  --nodeTypes NODETYPES
                        List of node types separated by commas. The syntax for
                        each node type depends on the provisioner used. For
                        the cgcloud and AWS provisioners this is the name of
                        an EC2 instance type, optionally followed by a colon
                        and the price in dollars to bid for a spot instance of
                        that type, for example 'c3.8xlarge:0.42'.If no spot
                        bid is specified, nodes of this type will be non-
                        preemptable.It is acceptable to specify an instance as
                        both preemptable and non-preemptable, including it
                        twice in the list. In that case,preemptable nodes of
                        that type will be preferred when creating new nodes
                        once the maximum number of preemptable-nodes has
                        beenreached.
  --nodeOptions NODEOPTIONS
                        Options for provisioning the nodes. The syntax depends
                        on the provisioner used. Neither the CGCloud nor the
                        AWS provisioner support any node options.
  --minNodes MINNODES   Mininum number of nodes of each type in the cluster,
                        if using auto-scaling. This should be provided as a
                        comma-separated list of the same length as the list of
                        node types. default=0
  --maxNodes MAXNODES   Maximum number of nodes of each type in the cluster,
                        if using autoscaling, provided as a comma-separated
                        list. The first value is used as a default if the list
                        length is less than the number of nodeTypes.
                        default=10
  --preemptableCompensation PREEMPTABLECOMPENSATION
                        The preference of the autoscaler to replace
                        preemptable nodes with non-preemptable nodes, when
                        preemptable nodes cannot be started for some reason.
                        Defaults to 0.0. This value must be between 0.0 and
                        1.0, inclusive. A value of 0.0 disables such
                        compensation, a value of 0.5 compensates two missing
                        preemptable nodes with a non-preemptable one. A value
                        of 1.0 replaces every missing pre-emptable node with a
                        non-preemptable one.
  --nodeStorage NODESTORAGE
                        Specify the size of the root volume of worker nodes
                        when they are launched in gigabytes. You may want to
                        set this if your jobs require a lot of disk space. The
                        default value is 50.
  --metrics             Enable the prometheus/grafana dashboard for monitoring
                        CPU/RAM usage, queue size, and issued jobs.
  --defaultMemory INT   The default amount of memory to request for a job.
                        Only applicable to jobs that do not specify an
                        explicit value for this requirement. Standard suffixes
                        like K, Ki, M, Mi, G or Gi are supported. Default is
                        2.0 Gi
  --defaultCores FLOAT  The default number of CPU cores to dedicate a job.
                        Only applicable to jobs that do not specify an
                        explicit value for this requirement. Fractions of a
                        core (for example 0.1) are supported on some batch
                        systems, namely Mesos and singleMachine. Default is
                        1.0
  --defaultDisk INT     The default amount of disk space to dedicate a job.
                        Only applicable to jobs that do not specify an
                        explicit value for this requirement. Standard suffixes
                        like K, Ki, M, Mi, G or Gi are supported. Default is
                        2.0 Gi
  --maxCores INT        The maximum number of CPU cores to request from the
                        batch system at any one time. Standard suffixes like
                        K, Ki, M, Mi, G or Gi are supported.
  --maxMemory INT       The maximum amount of memory to request from the batch
                        system at any one time. Standard suffixes like K, Ki,
                        M, Mi, G or Gi are supported.
  --maxDisk INT         The maximum amount of disk space to request from the
                        batch system at any one time. Standard suffixes like
                        K, Ki, M, Mi, G or Gi are supported.
  --retryCount RETRYCOUNT
                        Number of times to retry a failing job before giving
                        up and labeling job failed. default=1
  --maxJobDuration MAXJOBDURATION
                        Maximum runtime of a job (in seconds) before we kill
                        it (this is a lower bound, and the actual time before
                        killing the job may be longer).
  --rescueJobsFrequency RESCUEJOBSFREQUENCY
                        Period of time to wait (in seconds) between checking
                        for missing/overlong jobs, that is jobs which get lost
                        by the batch system.
  --maxServiceJobs MAXSERVICEJOBS
                        The maximum number of service jobs that can be run
                        concurrently, excluding service jobs running on
                        preemptable nodes. default=9223372036854775807
  --maxPreemptableServiceJobs MAXPREEMPTABLESERVICEJOBS
                        The maximum number of service jobs that can run
                        concurrently on preemptable nodes.
                        default=9223372036854775807
  --deadlockWait DEADLOCKWAIT
                        The minimum number of seconds to observe the cluster
                        stuck running only the same service jobs before
                        throwing a deadlock exception. default=60
  --statePollingWait STATEPOLLINGWAIT
                        Time, in seconds, to wait before doing a scheduler
                        query for job state. Return cached results if within
                        the waiting period.

  Miscellaneous options

  --disableCaching      Disables caching in the file store. This flag must be
                        set to use a batch system that does not support
                        caching such as Grid Engine, Parasol, LSF, or Slurm
  --disableChaining     Disables chaining of jobs (chaining uses one job's
                        resource allocation for its successor job if
                        possible).
  --maxLogFileSize MAXLOGFILESIZE
                        The maximum size of a job log file to keep (in bytes),
                        log files larger than this will be truncated to the
                        last X bytes. Setting this option to zero will prevent
                        any truncation. Setting this option to a negative
                        value will truncate from the beginning.Default=62.5 K
  --writeLogs [WRITELOGS]
                        Write worker logs received by the leader into their
                        own files at the specified path. The current working
                        directory will be used if a path is not specified
                        explicitly. Note: By default only the logs of failed
                        jobs are returned to leader. Set log level to 'debug'
                        to get logs back from successful jobs, and adjust
                        'maxLogFileSize' to control the truncation limit for
                        worker logs.
  --writeLogsGzip [WRITELOGSGZIP]
                        Identical to --writeLogs except the logs files are
                        gzipped on the leader.
  --realTimeLogging     Enable real-time logging from workers to masters
  --sseKey SSEKEY       Path to file containing 32 character key to be used
                        for server-side encryption on awsJobStore or
                        googleJobStore. SSE will not be used if this flag is
                        not passed.
  --cseKey CSEKEY       Path to file containing 256-bit key to be used for
                        client-side encryption on azureJobStore. By default,
                        no encryption is used.
  --setEnv NAME=VALUE or NAME, -e NAME=VALUE or NAME
                        Set an environment variable early on in the worker. If
                        VALUE is omitted, it will be looked up in the current
                        environment. Independently of this option, the worker
                        will try to emulate the leader's environment before
                        running a job. Using this option, a variable can be
                        injected into the worker process itself before it is
                        started.
  --servicePollingInterval SERVICEPOLLINGINTERVAL
                        Interval of time service jobs wait between polling for
                        the existence of the keep-alive flag (defailt=60)
